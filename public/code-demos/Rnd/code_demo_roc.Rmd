---
output: pdf_document
params:
  set_title: "Code Demo for ROC"
---
  
```{r roc-preamble, child = "../style/preamble_code_demos.Rmd", include = FALSE, purl = FALSE}

```

```{r roc-setup, child = "../style/setup.Rmd", include = FALSE, purl = FALSE}

```  

# Code demo for ROC

## Data


We use the Breast Cancer data set from [UCI database](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
and try to predict the class of the cancer.  
This is an **unbalanced data set**, and we manipulate the data set further to make it 
even more unbalanced. The data looks like that:

```{r roc-data, message=FALSE, warning=FALSE}
library("dplyr")
library("mlbench")
library("mlr3")
library("mlr3learners")
library("mlr3viz")

data("BreastCancer")

target_label <- "Class"

# delete one column with missing values
bc <- BreastCancer[, -c(1, 7)]

table(BreastCancer$Class) / nrow(BreastCancer)

# transform all factors to numeric, dangerous simplification but ok here
# (don't tell the stats profs...!)
mut <- bc[, -9] %>%
  mutate_all(as.character) %>%
  mutate_all(as.numeric)
bc_data <- cbind(mut, bc[, target_label])
colnames(bc_data) <- c(colnames(mut), target_label)
# make it more unbalanced and remove 60% of the "malignant" class instances
bc_data <- bc_data %>%
  filter(
    # always keep non-malignant
    (Class != "malignant") |
      # randomly discard non-malignant with probability .6
      ((Class == "malignant") & (runif(nrow(bc_data)) < .4))
  )

head(bc_data)
table(bc_data[, target_label]) / sum(table(bc_data[, target_label]))
```

We split the data again in train and test:

```{r roc-split_data}
# Data split
set.seed(1337)
train_size <- 3 / 4
train_indices <- sample(
  x = seq(1, nrow(bc_data), by = 1),
  size = ceiling(train_size * nrow(bc_data)), replace = FALSE
)
bc_train <- bc_data[ train_indices, ]
bc_test <- bc_data[ -train_indices, ]

task <- TaskClassif$new(
  id = "bc_task", backend = bc_train,
  target = target_label
)
```

## Models

We check the performance of three classifiers:

#### Logistic regression

```{r roc-logreg}
# logreg
learner_logreg <- lrn("classif.log_reg", predict_type = "prob")
learner_logreg$train(task)
pred_logreg <- learner_logreg$predict_newdata(newdata = bc_test)
pred_logreg$score(list(msr("classif.ce"), msr("classif.auc")))
pred_logreg$confusion
```

#### $k$-NN

```{r roc-knn}
# knn
learner_knn <- lrn("classif.kknn", k = 5, predict_type = "prob")
learner_knn$train(task)
pred_knn <- learner_knn$predict_newdata(newdata = bc_test)
pred_knn$score(list(msr("classif.ce"), msr("classif.auc")))
pred_knn$confusion
```

####  Featureless 

.. a fairly stupid learner that simply predicts the majority of the two classes for each observation. 

```{r roc-maj_vote}

# learner that uses simple majority vote for classification
learner_stupid <- lrn("classif.featureless",
  method = "mode", predict_type = "prob"
)

learner_stupid$train(task)
pred_stupid <- learner_stupid$predict_newdata(newdata = bc_test)
pred_stupid$score(list(msr("classif.ce"), msr("classif.auc")))
pred_stupid$confusion
```

By looking at the confusion matrices we see that the problem is now, that even the stupid approach yields a reasonable mmce performance. Thus, we need additional measure: Let's compare the logistic regression and the stupid learner in terms of sensitivity[^1] and specificity[^2] (check if you can compute these values by hand): 

[^1]: Also called *true positive rate* or *recall*.
[^2]: Also called *true negative rate*

## ROC Curve Evaluation


```{r roc-conf}
pred_logreg$confusion
pred_logreg$score(list(msr("classif.sensitivity"), msr("classif.specificity")))

pred_stupid$confusion
pred_stupid$score(list(msr("classif.sensitivity"), msr("classif.specificity")))
```

A specificity of 0 means that all ill persons would be told they are healthy, which is certainly not what the 
test is intended for. On the other hand can we do better with the logistic regression in terms of those measures?
Remember with our classification methods we try to estimate the posterior probabilities. Until now in the case of two classes we classified the observation as the first class if its posterior probability is greater or equal to 50% and otherwise as the second class. So what happens if we move this threshold?

```{r roc-thrsh}
pred_logreg$set_threshold(0.01)
pred_logreg$confusion
pred_logreg$score(list(msr("classif.sensitivity"), msr("classif.specificity")))

pred_logreg$set_threshold(0.7)
pred_logreg$confusion
pred_logreg$score(list(msr("classif.sensitivity"), msr("classif.specificity")))
```

We see that in our case with a higher threshold value the specificity improves and 
the sensitivity degrades and vice versa. We can investigate this relationship with 
ROC curves. Compare the ROC curves:

```{r roc-roc_curves}
task <- TaskClassif$new(
  id = "bc_all_task", backend = bc_data,
  target = target_label
)
learners <- list(learner_logreg, learner_knn, learner_stupid)

design <- benchmark_grid(
  tasks = task,
  learners = learners,
  resamplings = rsmp("cv", folds = 3)
)
bmr <- benchmark(design)
autoplot(bmr, type = "roc")
```
